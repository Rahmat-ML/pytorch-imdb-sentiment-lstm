{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-Step Sentiment Analysis with PyTorch and LSTM\n",
    "\n",
    "Welcome! In this notebook, we will build a complete sentiment analysis model using PyTorch. We'll use the IMDB movie review dataset to classify reviews as either positive or negative.\n",
    "\n",
    "We will follow these steps:\n",
    "1.  **Import Libraries:** Load all necessary packages.\n",
    "2.  **Load Data:** Read the `imdb_tr.csv` and `test/imdb_te.csv` files.\n",
    "3.  **Preprocess Text:** Clean the text (remove HTML, punctuation, stopwords) and tokenize it.\n",
    "4.  **Build Vocabulary:** Create a word-to-index mapping from our training data.\n",
    "5.  **Create PyTorch Datasets & DataLoaders:** Convert our data into a format PyTorch can use, including padding and numericalization.\n",
    "6.  **Define the LSTM Model:** Create our neural network architecture.\n",
    "7.  **Train the Model:** Feed the data to the model and update its weights.\n",
    "8.  **Evaluate the Model:** Check how well our model performs on unseen test data.\n",
    "9.  **Run Inference:** Use the trained model to predict the sentiment of new, custom reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download NLTK data (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Data\n",
    "\n",
    "We'll load our training and testing data from the CSV files using pandas. The `imdb_tr.csv` file will be used for both training and validation, and `test/imdb_te.csv` will be reserved for our final test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "try:\n",
    "    train_val_df = pd.read_csv('imdb_tr.csv')\n",
    "    test_df = pd.read_csv('test/imdb_te.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Make sure 'imdb_tr.csv' and 'test/imdb_te.csv' are in the correct directories.\")\n",
    "    # As a fallback, try loading from the root (in case 'test' isn't a subdir)\n",
    "    try:\n",
    "        test_df = pd.read_csv('imdb_te.csv')\n",
    "    except FileNotFoundError:\n",
    "        print(\"Could not find 'imdb_te.csv' in root either.\")\n",
    "\n",
    "print(\"Training/Validation Data Head:\")\n",
    "print(train_val_df.head())\n",
    "print(\"\\nTest Data Head:\")\n",
    "print(test_df.head())\n",
    "\n",
    "print(f\"\\nTotal training/validation samples: {len(train_val_df)}\")\n",
    "print(f\"Total test samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Preprocess Text\n",
    "\n",
    "This is a crucial step. We need to clean the text to make it easier for the model to learn.\n",
    "Our `preprocess_text` function will:\n",
    "1.  Remove HTML tags (like `<br />`).\n",
    "2.  Remove punctuation and special characters, keeping only letters.\n",
    "3.  Convert all text to lowercase.\n",
    "4.  Tokenize the text (split it into individual words).\n",
    "5.  Remove common English stopwords (like 'a', 'the', 'is')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 1. Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # 2. Remove punctuation/special chars\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "    # 3. Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # 4. Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # 5. Remove stopwords\n",
    "    processed_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return processed_tokens\n",
    "\n",
    "# Let's test the function\n",
    "sample_review = \"This is a sample review... <br /><br />It's not bad, but it could be better! 10/10\"\n",
    "print(f\"Original: {sample_review}\")\n",
    "print(f\"Processed: {preprocess_text(sample_review)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build Vocabulary\n",
    "\n",
    "Our model can't understand words; it only understands numbers. We need to create a \"vocabulary\" that maps each unique word to an integer.\n",
    "\n",
    "We'll add two special tokens:\n",
    "* `<PAD>`: A token we'll use to pad shorter reviews so all sequences in a batch have the same length.\n",
    "* `<UNK>`: A token for words that appear in our test data but not in our training data (unknown words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(data, min_freq=5):\n",
    "    word_counts = Counter()\n",
    "    for text in data:\n",
    "        word_counts.update(text)\n",
    "    \n",
    "    # Create vocabulary, starting with special tokens\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    \n",
    "    # Add words that meet the minimum frequency
    "    idx = 2\n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_freq:\n",
    "            vocab[word] = idx\n",
    "            idx += 1\n",
    "    return vocab\n",
    "\n",
    "# Apply preprocessing to our training data before building vocab\n",
    "print(\"Preprocessing training data (this may take a minute)...\")\n",
    "# We use train_val_df['text'] to build the vocab\n",
    "processed_train_texts = [preprocess_text(text) for text in tqdm(train_val_df['text'])]\n",
    "\n",
    "# Build the vocabulary\n",
    "word2idx = build_vocab(processed_train_texts)\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "print(f\"\\nVocabulary size: {vocab_size}\")\n",
    "print(\"First 10 vocab items:\", list(word2idx.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create PyTorch Datasets & DataLoaders\n",
    "\n",
    "Now we'll create a custom `SentimentDataset` class. This class will handle:\n",
    "1.  Taking a review text.\n",
    "2.  Preprocessing it.\n",
    "3.  Converting its words to indices using our `word2idx` vocabulary.\n",
    "4.  **Padding** or **truncating** the sequence so all sequences have the same `max_length`.\n",
    "\n",
    "We'll also split our `train_val_df` into a training set and a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, word2idx, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.word2idx = word2idx\n",
    "        self.max_length = max_length\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "        text = text.lower()\n",
    "        tokens = word_tokenize(text)\n",
    "        processed_tokens = [word for word in tokens if word not in self.stop_words]\n",
    "        return processed_tokens\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        processed_tokens = self.preprocess_text(text)\n",
    "        \n",
    "        # Convert to indices\n",
    "        indexed_tokens = [self.word2idx.get(word, self.word2idx['<UNK>']) for word in processed_tokens]\n",
    "        \n",
    "        # Pad or truncate\n",
    "        if len(indexed_tokens) < self.max_length:\n",
    "            # Pad with <PAD> token (index 0)\n",
    "            padded_tokens = indexed_tokens + [self.word2idx['<PAD>']] * (self.max_length - len(indexed_tokens))\n",
    "        else:\n",
    "            # Truncate\n",
    "            padded_tokens = indexed_tokens[:self.max_length]\n",
    "            \n",
    "        return torch.tensor(padded_tokens), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "MAX_LENGTH = 200\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Split training data into train and validation sets\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.2, random_state=42, stratify=train_val_df['s'])\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = SentimentDataset(train_df['text'].tolist(), train_df['s'].tolist(), word2idx, MAX_LENGTH)\n",
    "val_dataset = SentimentDataset(val_df['text'].tolist(), val_df['s'].tolist(), word2idx, MAX_LENGTH)\n",
    "test_dataset = SentimentDataset(test_df['text'].tolist(), test_df['s'].tolist(), word2idx, MAX_LENGTH)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Define the LSTM Model\n",
    "\n",
    "Time to build our network! It will have:\n",
    "1.  `nn.Embedding`: Turns our word indices into dense vectors (embeddings).\n",
    "2.  `nn.LSTM`: The main recurrent layer that processes the sequence.\n",
    "3.  `nn.Linear`: A standard fully-connected layer to give us a final score.\n",
    "4.  `nn.Sigmoid`: (Applied in the forward pass) To squash the score between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout, padding_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            num_layers=n_layers, \n",
    "                            bidirectional=True,  # Bidirectional LSTM\n",
    "                            dropout=dropout, \n",
    "                            batch_first=True) # Expects (batch_size, seq_len, features)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Linear layer\n",
    "        # We use hidden_dim * 2 because it's bidirectional\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        # Sigmoid activation\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text_batch):\n",
    "        # text_batch shape: [BATCH_SIZE, MAX_LENGTH]\n",
    "        \n",
    "        # 1. Embedding\n",
    "        embedded = self.embedding(text_batch)\n",
    "        # embedded shape: [BATCH_SIZE, MAX_LENGTH, EMBEDDING_DIM]\n",
    "        \n",
    "        # 2. LSTM\n",
    "        # packed_output, (hidden_state, cell_state)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        # lstm_out shape: [BATCH_SIZE, MAX_LENGTH, HIDDEN_DIM * 2]\n",
    "        # hidden shape: [N_LAYERS * 2, BATCH_SIZE, HIDDEN_DIM]\n",
    "        \n",
    "        # 3. Concatenate the final forward and backward hidden states\n",
    "        # We take the last layer's hidden state (forward and backward)\n",
    "        # hidden[-2,:,:] is the final forward hidden state\n",
    "        # hidden[-1,:,:] is the final backward hidden state\n",
    "        hidden_concat = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        # hidden_concat shape: [BATCH_SIZE, HIDDEN_DIM * 2]\n",
    "        \n",
    "        # 4. Dropout and Linear Layer\n",
    "        dropped_out = self.dropout(hidden_concat)\n",
    "        linear_out = self.fc(dropped_out)\n",
    "        # linear_out shape: [BATCH_SIZE, OUTPUT_DIM]\n",
    "        \n",
    "        # 5. Sigmoid activation\n",
    "        sig_out = self.sigmoid(linear_out)\n",
    "        \n",
    "        # Squeeze to remove extra dimension\n",
    "        return sig_out.squeeze()\n",
    "\n",
    "\n",
    "# --- Model Hyperparameters ---\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "PADDING_IDX = word2idx['<PAD>']\n",
    "\n",
    "# Instantiate the model\n",
    "model = SentimentLSTM(vocab_size, \n",
    "                        EMBEDDING_DIM, \n",
    "                        HIDDEN_DIM, \n",
    "                        OUTPUT_DIM, \n",
    "                        N_LAYERS, \n",
    "                        DROPOUT, \n",
    "                        PADDING_IDX)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Hyperparameters ---\n",
    "LEARNING_RATE = 0.001\n",
    "N_EPOCHS = 5\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCELoss() # Binary Cross Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "def get_accuracy(preds, y):\n",
    "    \"\"\"Returns accuracy per batch\"\"\"\n",
    "    # Round predictions to the closest integer (0 or 1)\n",
    "    rounded_preds = torch.round(preds)\n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    \n",
    "    # --- Training Phase ---\n",
    "    model.train() # Set model to training mode\n",
    "    \n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{N_EPOCHS} [Train]\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # 1. Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Forward pass\n",
    "        predictions = model(inputs)\n",
    "        \n",
    "        # 3. Calculate loss and accuracy\n",
    "        loss = criterion(predictions, labels)\n",
    "        acc = get_accuracy(predictions, labels)\n",
    "        \n",
    "        # 4. Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_acc += acc.item()\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    \n",
    "    model.eval() # Set model to evaluation mode\n",
    "    with torch.no_grad(): # Disable gradient calculation\n",
    "        for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{N_EPOCHS} [Val]\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            predictions = model(inputs)\n",
    "            \n",
    "            loss = criterion(predictions, labels)\n",
    "            acc = get_accuracy(predictions, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_acc += acc.item()\n",
    "            \n",
    "    # Print epoch statistics\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_train_acc = train_acc / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_val_acc = val_acc / len(val_loader)\n",
    "    \n",
    "    print(f'Epoch {epoch+1:02} | Train Loss: {avg_train_loss:.3f} | Train Acc: {avg_train_acc*100:.2f}% | Val. Loss: {avg_val_loss:.3f} | Val. Acc: {avg_val_acc*100:.2f}%')\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate the Model\n",
    "\n",
    "Now we'll use the held-out test set (`test_loader`) to see how our model generalizes to completely new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0.0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        predictions = model(inputs)\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # Store predictions and labels\n",
    "        rounded_preds = torch.round(predictions)\n",
    "        all_preds.extend(rounded_preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "test_acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "print(f'Test Loss: {avg_test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "print(\"\\nClassification Report:\")\n",
    "# 0 is negative, 1 is positive\n",
    "print(classification_report(all_labels, all_preds, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Run Inference\n",
    "\n",
    "Let's create a final function that takes any review as a string and predicts its sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review_text, model, word2idx, max_length, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess the text\n",
    "    processed_tokens = preprocess_text(review_text)\n",
    "    \n",
    "    # Convert to indices\n",
    "    indexed_tokens = [word2idx.get(word, word2idx['<UNK>']) for word in processed_tokens]\n",
    "    \n",
    "    # Pad or truncate\n",
    "    if len(indexed_tokens) < max_length:\n",
    "        padded_tokens = indexed_tokens + [word2idx['<PAD>']] * (max_length - len(indexed_tokens))\n",
    "    else:\n",
    "        padded_tokens = indexed_tokens[:max_length]\n",
    "        \n",
    "    # Convert to tensor and add batch dimension (batch size = 1)\n",
    "    input_tensor = torch.tensor(padded_tokens).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_tensor)\n",
    "        \n",
    "    probability = prediction.item()\n",
    "    sentiment = \"Positive\" if probability > 0.5 else \"Negative\"\n",
    "    \n",
    "    return sentiment, probability\n",
    "\n",
    "# --- Test with new reviews ---\n",
    "\n",
    "positive_review = \"This movie was fantastic! The acting was superb and the plot was gripping. I would recommend this to everyone.\"\n",
    "neg_review = \"What a waste of time. The plot was predictable and the acting was terrible. I would not watch this again.\"\n",
    "\n",
    "sentiment, prob = predict_sentiment(positive_review, model, word2idx, MAX_LENGTH, device)\n",
    "print(f\"Review: '{positive_review}'\")\n",
    "print(f\"Sentiment: {sentiment} (Probability: {prob:.4f})\\n\")\n",
    "\n",
    "sentiment, prob = predict_sentiment(neg_review, model, word2idx, MAX_LENGTH, device)\n",
    "print(f\"Review: '{neg_review}'\")\n",
    "print(f\"Sentiment: {sentiment} (Probability: {prob:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
